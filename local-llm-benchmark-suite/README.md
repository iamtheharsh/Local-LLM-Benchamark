# Local LLM Benchmark Suite

A comprehensive testing interface for evaluating local Large Language Models (LLMs) performance, resource consumption, and capabilities on macOS.

## Overview

This application provides a three-panel interface for:
- **Chat Interface**: Test local LLMs with multimodal inputs
- **Agentic Capabilities**: Manage tools and MCP servers
- **RAG**: Manage vector databases and retrieval-augmented generation
- **Resource Monitoring**: Track RAM, CPU, and battery consumption
- **Event Logging**: Comprehensive operation logs with filtering

## Prerequisites

- Node.js (version 16 or higher)
- Rust (latest stable version)
- Xcode Command Line Tools (on macOS)

## Installation

1. Install dependencies:
```bash
npm install
```

2. Install Rust dependencies:
```bash
cd src-tauri
cargo build
cd ..
```

## Running the Application

### Development Mode

Start the development server:
```bash
npm run tauri dev
```

This will:
- Start the Vite development server on http://localhost:1420
- Launch the Tauri application window
- Enable hot reload for both frontend and backend

### Building for Production

Build the application:
```bash
npm run tauri build
```

This creates a distributable app in `src-tauri/target/release/bundle/`.

## Project Structure

```
local-llm-benchmark-suite/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ChatPanel.jsx          # Chat interface component
â”‚   â”‚   â”œâ”€â”€ AgenticPanel.jsx       # Agentic capabilities tab
â”‚   â”‚   â”œâ”€â”€ RAGPanel.jsx           # RAG management tab
â”‚   â”‚   â”œâ”€â”€ ToolsPanel.jsx         # Tools management tab
â”‚   â”‚   â””â”€â”€ MCPPanel.jsx           # MCP management tab
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ AgentRuntime.js        # Agentic runtime engine
â”‚   â”‚   â”œâ”€â”€ MemoryManager.js       # Memory/context management
â”‚   â”‚   â””â”€â”€ BenchmarkManager.js    # Metrics collection engine
â”‚   â”œâ”€â”€ context/
â”‚   â”‚   â”œâ”€â”€ ToolContext.jsx        # Tool state management
â”‚   â”‚   â””â”€â”€ MemoryContext.jsx      # Memory state management
â”‚   â”œâ”€â”€ panels/
â”‚   â”‚   â”œâ”€â”€ ResourceMonitor.jsx    # Resource monitoring display
â”‚   â”‚   â”œâ”€â”€ LogsPanel.jsx          # Event logging display
â”‚   â”‚   â””â”€â”€ BenchmarkDashboard.jsx # Analytics dashboard
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ metrics.js             # System metrics utilities
â”‚   â”‚   â””â”€â”€ logger.js              # Logging utility
â”‚   â”œâ”€â”€ App.jsx                    # Main application component
â”‚   â””â”€â”€ main.jsx                   # React entry point
â”œâ”€â”€ src-tauri/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ main.rs                # Rust backend entry point
â”‚   â”œâ”€â”€ Cargo.toml                 # Rust dependencies
â”‚   â””â”€â”€ build.rs                   # Build configuration
â”œâ”€â”€ tauri.conf.json                # Tauri configuration
â”œâ”€â”€ package.json                   # Node.js dependencies
â”œâ”€â”€ vite.config.js                 # Vite configuration
â””â”€â”€ index.html                     # HTML entry point
```

## Features

### Phase 1-10 âœ… COMPLETED
- âœ… Basic project structure
- âœ… Three-panel layout (Left: Tabs, Center: Resources, Right: Logs)
- âœ… Tab navigation system (Chat, Agentic, RAG, Tools, MCP, Benchmark)
- âœ… Dark theme UI with professional styling
- âœ… Modular component architecture

### Phase 2: Chat Interface âœ…
- âœ… Functional chat UI with message history
- âœ… Simulated LLM responses with latency tracking
- âœ… Token counting and metrics display
- âœ… Real-time thinking indicator
- âœ… Scroll-to-bottom and auto-scroll
- âœ… Agentic tool integration
- âœ… RAG context display

### Phase 3: System Resources âœ…
- âœ… Live resource monitoring (RAM/CPU/Battery)
- âœ… Real-time metrics with threshold warnings
- âœ… Visual progress bars with color coding
- âœ… Polling system (1-second intervals)
- âœ… Tauri backend integration (with web fallback)

### Phase 4: Event Logging âœ…
- âœ… Comprehensive event logging system
- âœ… Multi-level logs (info, debug, warning, error)
- âœ… Category-based filtering
- âœ… Timestamps and log management
- âœ… Console integration with custom logger

### Phase 5: Tools Management âœ…
- âœ… Tool creation and editing interface
- âœ… HTTP tool configuration (GET, POST, PUT, DELETE)
- âœ… JSON schema validation for variables and headers
- âœ… Tool testing with latency measurement
- âœ… Response size and status tracking
- âœ… Enable/disable toggles

### Phase 7: MCP Integration âœ…
- âœ… MCP server management UI
- âœ… Server connection with latency measurement
- âœ… Tool discovery from connected servers
- âœ… Authentication token support
- âœ… Connect/disconnect functionality
- âœ… Simulated tool execution testing
- âœ… Integration hook for agentic pipelines

### Phase 8: Agentic Runtime âœ…
- âœ… Intent detection with keyword matching
- âœ… Multi-strategy tool matching
- âœ… Automatic tool invocation
- âœ… Latency measurement and tracking

### Phase 9: Memory Management âœ…
- âœ… Chunk-based memory storage
- âœ… Text similarity matching
- âœ… RAG context retrieval
- âœ… Memory integration with chat

### Phase 10: Benchmark Dashboard âœ…
- âœ… Real-time metrics collection
- âœ… Interactive analytics dashboard
- âœ… Visual charts (line, area charts)
- âœ… Time window filters
- âœ… Export functionality (JSON/CSV)
- âœ… Performance tracking (latency, throughput, success rate)

## Development

This project uses:
- **Tauri** for native desktop app framework
- **React** for UI components
- **Vite** for fast development and building
- **Rust** for backend operations

## Troubleshooting

### Issue: Permission denied when running `npm run tauri dev`
**Solution**: Make sure Xcode Command Line Tools are installed:
```bash
xcode-select --install
```

### Issue: Rust compilation errors
**Solution**: Update Rust to latest stable version:
```bash
rustup update stable
```

### Issue: Module not found errors
**Solution**: Clear node_modules and reinstall:
```bash
rm -rf node_modules package-lock.json
npm install
```

## License

MIT

## Contributing

This is a personal project for benchmarking local LLMs. Feel free to fork and modify for your needs.

## Current Status (November 2025)

### ðŸŽ‰ Phase 10 Complete: Benchmark Metrics & Analytics Dashboard

The Local LLM Benchmark Suite is now a comprehensive **local AI agent framework** with complete benchmarking and analytics capabilities:

**âœ… Fully Implemented:**
- Six-tab interface (Chat, Agentic, RAG, Tools, MCP, Benchmark)
- Real-time system resource monitoring
- Comprehensive event logging
- Tool management with HTTP API support
- MCP server connectivity and tool discovery
- Agentic runtime with intent detection
- Memory management and RAG context
- **Analytics dashboard with charts and metrics export**
- Production-ready build system

**ðŸ“Š Benchmarking Capabilities:**
- Track latency, throughput, and success rates
- Visual charts with Recharts library
- Time window filtering (5min, 30min, 1hr, 24hr)
- Export metrics as JSON or CSV
- Memory usage tracking
- CPU correlation analysis

**ðŸš€ Access the Application:**
```bash
# Development mode (with hot reload)
npm run dev

# Build for production
npm run build
```

Visit http://localhost:1420/ to use the application.

**ðŸ“Š Architecture:**
The application is now a complete **local AI benchmarking platform** that can:
- Connect to multiple MCP servers simultaneously
- Manage and test HTTP-based tools
- Monitor system resources in real-time
- Track performance metrics across all operations
- Log all operations with comprehensive filtering
- Export benchmark data for analysis
- Provide a foundation for agentic AI workflows

This positions the suite as a robust **local alternative to cloud-based AI agents** with full benchmarking capabilities for comparative LLM analysis.
